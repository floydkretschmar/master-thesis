{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of \"Fair Decisions Despite Imperfect Predictions\" using the original fairness constraint"
   ]
  },
  {
   "source": [
    "import numpy as np\n",
    "from src.consequential_learning import collect_data, train\n",
    "from src.feature_map import IdentityFeatureMap"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The parameters used by the the original authors  \n",
    "Note: Learning rate decay is not yet implemented"
   ]
  },
  {
   "source": [
    "training_parameters = {\n",
    "    'dim_x': 1,\n",
    "    'dim_s': 1,\n",
    "    'time_steps':200,\n",
    "    'batch_size':512,\n",
    "    'num_iterations': 32,\n",
    "    'learning_parameters': {\n",
    "        'learning_rate': 0.5,\n",
    "        'decay_rate': 0.8,\n",
    "        'decay_step': 30\n",
    "    },\n",
    "    'fairness_rate':0,\n",
    "    'cost_factor':0.55,\n",
    "    'fraction_protected':0.3\n",
    "}\n",
    "training_parameters['dim_theta'] = training_parameters['dim_x'] + training_parameters['dim_s']\n",
    "training_parameters['feature_map'] = IdentityFeatureMap(training_parameters['dim_theta'])\n",
    "training_parameters['num_decisions'] = training_parameters['num_iterations'] * training_parameters['batch_size']"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of the fairness function  \n",
    "The original fairness constraint was defined as the difference of benefits $b_{P}^s$ for both of the settings of the protected attribute. This function defines both the fairness function as well as its derivative which is controlled by the parameter gradient=true/false"
   ]
  },
  {
   "source": [
    "def benefit_function(x_s, s, sample_theta, policy, gradient):\n",
    "    ips_weight, phi, log_gradient_denominator = policy.calculate_ips_weights_and_log_gradient(x_s, s, sample_theta)\n",
    "    decision = policy(x_s, s).reshape(-1, 1)\n",
    "\n",
    "    if gradient:\n",
    "        grad_benefit = ((ips_weight/log_gradient_denominator) * decision * phi).sum(axis=0) / x_s.shape[0]\n",
    "        return grad_benefit\n",
    "    else:\n",
    "        benefit = (ips_weight * decision).sum(axis=0) / x_s.shape[0]\n",
    "        return benefit\n",
    "\n",
    "def fairness_function(**fairness_kwargs):\n",
    "    x = fairness_kwargs['x']\n",
    "    s = fairness_kwargs['s']\n",
    "    sample_theta = fairness_kwargs['sample_theta']\n",
    "    policy = fairness_kwargs['policy']\n",
    "    gradient = fairness_kwargs['gradient']\n",
    "\n",
    "    pos_decision_idx = np.arange(s.shape[0]).reshape(-1, 1)\n",
    "\n",
    "    s_0_idx = pos_decision_idx[s == 0]\n",
    "    s_1_idx = pos_decision_idx[s == 1]\n",
    "\n",
    "    return benefit_function(x[s_0_idx], s[s_0_idx], sample_theta, policy, gradient) - benefit_function(x[s_1_idx], s[s_1_idx], sample_theta, policy, gradient)\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model"
   ]
  },
  {
   "source": [
    "train(**training_parameters, fairness_function=fairness_function)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time step 1: Utility -0.011618307636275365\nLearning rate: 0.5\nTime step 2: Utility 0.04894313377880811\nLearning rate: 0.5\nTime step 3: Utility 0.0562976579520697\nLearning rate: 0.5\nTime step 4: Utility 0.05628861732802415\nLearning rate: 0.5\nTime step 5: Utility 0.055879917184265\nLearning rate: 0.5\nTime step 6: Utility 0.051756994188403814\nLearning rate: 0.5\nTime step 7: Utility 0.06651767294732497\nLearning rate: 0.5\nTime step 8: Utility 0.06091082889526148\nLearning rate: 0.5\nTime step 9: Utility 0.059982247712686045\nLearning rate: 0.5\nTime step 10: Utility 0.05611111111111109\nLearning rate: 0.5\nTime step 11: Utility 0.06907008086253368\nLearning rate: 0.5\nTime step 12: Utility 0.07323517269850006\nLearning rate: 0.5\nTime step 13: Utility 0.06399324324324322\nLearning rate: 0.5\nTime step 14: Utility 0.07058226134055515\nLearning rate: 0.5\nTime step 15: Utility 0.06463153671258524\nLearning rate: 0.5\nTime step 16: Utility 0.06244768462265422\nLearning rate: 0.5\nTime step 17: Utility 0.06165373294086164\nLearning rate: 0.5\nTime step 18: Utility 0.06780652263657629\nLearning rate: 0.5\nTime step 19: Utility 0.06726382179280728\nLearning rate: 0.5\nTime step 20: Utility 0.07447987955105391\nLearning rate: 0.5\nTime step 21: Utility 0.07211852861035421\nLearning rate: 0.5\nTime step 22: Utility 0.07248813559322033\nLearning rate: 0.5\nTime step 23: Utility 0.07220318069646282\nLearning rate: 0.5\nTime step 24: Utility 0.06535557386051617\nLearning rate: 0.5\nTime step 25: Utility 0.06366168478260867\nLearning rate: 0.5\nTime step 26: Utility 0.07423082182414877\nLearning rate: 0.5\nTime step 27: Utility 0.07074560815341288\nLearning rate: 0.5\nTime step 28: Utility 0.07222895622895623\nLearning rate: 0.5\nTime step 29: Utility 0.06353035786087655\nLearning rate: 0.5\nTime step 30: Utility 0.08031342480495021\nLearning rate: 0.4\nTime step 31: Utility 0.07407781367797521\nLearning rate: 0.4\nTime step 32: Utility 0.07430555555555554\nLearning rate: 0.4\nTime step 33: Utility 0.06356932153392329\nLearning rate: 0.4\nTime step 34: Utility 0.06652490886998783\nLearning rate: 0.4\nTime step 35: Utility 0.06628215327499336\nLearning rate: 0.4\nTime step 36: Utility 0.0639587794432548\nLearning rate: 0.4\nTime step 37: Utility 0.07653116166375015\nLearning rate: 0.4\nTime step 38: Utility 0.07853172451193056\nLearning rate: 0.4\nTime step 39: Utility 0.07149993157246473\nLearning rate: 0.4\nTime step 40: Utility 0.0653794129582037\nLearning rate: 0.4\nTime step 41: Utility 0.07469784550709405\nLearning rate: 0.4\nTime step 42: Utility 0.07254009241641747\nLearning rate: 0.4\nTime step 43: Utility 0.06938843637334756\nLearning rate: 0.4\nTime step 44: Utility 0.07987565887282064\nLearning rate: 0.4\nTime step 45: Utility 0.07476241467005754\nLearning rate: 0.4\nTime step 46: Utility 0.06881254169446296\nLearning rate: 0.4\nTime step 47: Utility 0.061089441829186256\nLearning rate: 0.4\nTime step 48: Utility 0.07069355697550583\nLearning rate: 0.4\nTime step 49: Utility 0.07510790396547072\nLearning rate: 0.4\nTime step 50: Utility 0.07803598200899547\nLearning rate: 0.4\nTime step 51: Utility 0.07864652648389671\nLearning rate: 0.4\nTime step 52: Utility 0.06912252754546659\nLearning rate: 0.4\nTime step 53: Utility 0.07589923300714096\nLearning rate: 0.4\nTime step 54: Utility 0.07242864835756595\nLearning rate: 0.4\nTime step 55: Utility 0.07293565860747785\nLearning rate: 0.4\nTime step 56: Utility 0.07917276233541694\nLearning rate: 0.4\nTime step 57: Utility 0.079209758684699\nLearning rate: 0.4\nTime step 58: Utility 0.0695060080106809\nLearning rate: 0.4\nTime step 59: Utility 0.06821653490553395\nLearning rate: 0.4\nTime step 60: Utility 0.080037513397642\nLearning rate: 0.32000000000000006\nTime step 61: Utility 0.06789459532132291\nLearning rate: 0.32000000000000006\nTime step 62: Utility 0.07299328859060403\nLearning rate: 0.32000000000000006\nTime step 63: Utility 0.0686986946575158\nLearning rate: 0.32000000000000006\nTime step 64: Utility 0.07082152974504247\nLearning rate: 0.32000000000000006\nTime step 65: Utility 0.06913744834022129\nLearning rate: 0.32000000000000006\nTime step 66: Utility 0.07477639834468026\nLearning rate: 0.32000000000000006\nTime step 67: Utility 0.06782515706456355\nLearning rate: 0.32000000000000006\nTime step 68: Utility 0.07294851166532582\nLearning rate: 0.32000000000000006\nTime step 69: Utility 0.06820703653585924\nLearning rate: 0.32000000000000006\nTime step 70: Utility 0.07849636216653191\nLearning rate: 0.32000000000000006\nTime step 71: Utility 0.077522874058127\nLearning rate: 0.32000000000000006\nTime step 72: Utility 0.07276433631867396\nLearning rate: 0.32000000000000006\nTime step 73: Utility 0.07669413919413919\nLearning rate: 0.32000000000000006\nTime step 74: Utility 0.0719691895512391\nLearning rate: 0.32000000000000006\nTime step 75: Utility 0.07012661566868898\nLearning rate: 0.32000000000000006\nTime step 76: Utility 0.07293443496801703\nLearning rate: 0.32000000000000006\nTime step 77: Utility 0.06629826715786599\nLearning rate: 0.32000000000000006\nTime step 78: Utility 0.07685209949184271\nLearning rate: 0.32000000000000006\nTime step 79: Utility 0.07566258576617783\nLearning rate: 0.32000000000000006\nTime step 80: Utility 0.0779744279946164\nLearning rate: 0.32000000000000006\nTime step 81: Utility 0.07495371594816184\nLearning rate: 0.32000000000000006\nTime step 82: Utility 0.07092334035827184\nLearning rate: 0.32000000000000006\nTime step 83: Utility 0.0667451244115669\nLearning rate: 0.32000000000000006\nTime step 84: Utility 0.0759747243882764\nLearning rate: 0.32000000000000006\nTime step 85: Utility 0.08569169960474307\nLearning rate: 0.32000000000000006\nTime step 86: Utility 0.07264396115442481\nLearning rate: 0.32000000000000006\nTime step 87: Utility 0.06825783513299971\nLearning rate: 0.32000000000000006\nTime step 88: Utility 0.07277313155449354\nLearning rate: 0.32000000000000006\nTime step 89: Utility 0.06915575741383914\nLearning rate: 0.32000000000000006\nTime step 90: Utility 0.073744966442953\nLearning rate: 0.25600000000000006\nTime step 91: Utility 0.0740143369175627\nLearning rate: 0.25600000000000006\nTime step 92: Utility 0.0666869095816464\nLearning rate: 0.25600000000000006\nTime step 93: Utility 0.07418076819769003\nLearning rate: 0.25600000000000006\nTime step 94: Utility 0.07230636375714093\nLearning rate: 0.25600000000000006\nTime step 95: Utility 0.07661999999999998\nLearning rate: 0.25600000000000006\nTime step 96: Utility 0.08091200427293362\nLearning rate: 0.25600000000000006\nTime step 97: Utility 0.08179149269311063\nLearning rate: 0.25600000000000006\nTime step 98: Utility 0.07490005330490403\nLearning rate: 0.25600000000000006\nTime step 99: Utility 0.07330026455026453\nLearning rate: 0.25600000000000006\nTime step 100: Utility 0.0689710183461845\nLearning rate: 0.25600000000000006\nTime step 101: Utility 0.06297434526990912\nLearning rate: 0.25600000000000006\nTime step 102: Utility 0.06971356916578668\nLearning rate: 0.25600000000000006\nTime step 103: Utility 0.07816417306914676\nLearning rate: 0.25600000000000006\nTime step 104: Utility 0.06684506288466682\nLearning rate: 0.25600000000000006\nTime step 105: Utility 0.07392809587217042\nLearning rate: 0.25600000000000006\nTime step 106: Utility 0.07680074836295601\nLearning rate: 0.25600000000000006\nTime step 107: Utility 0.06764940769333154\nLearning rate: 0.25600000000000006\nTime step 108: Utility 0.08300371648526678\nLearning rate: 0.25600000000000006\nTime step 109: Utility 0.06752816434724981\nLearning rate: 0.25600000000000006\nTime step 110: Utility 0.0738436268068331\nLearning rate: 0.25600000000000006\nTime step 111: Utility 0.0713207799145299\nLearning rate: 0.25600000000000006\nTime step 112: Utility 0.0632256772988122\nLearning rate: 0.25600000000000006\nTime step 113: Utility 0.06869559482986018\nLearning rate: 0.25600000000000006\nTime step 114: Utility 0.07656979069457405\nLearning rate: 0.25600000000000006\nTime step 115: Utility 0.07794733288318702\nLearning rate: 0.25600000000000006\nTime step 116: Utility 0.07636171337916443\nLearning rate: 0.25600000000000006\nTime step 117: Utility 0.07299705251875668\nLearning rate: 0.25600000000000006\nTime step 118: Utility 0.07466471916080201\nLearning rate: 0.25600000000000006\nTime step 119: Utility 0.07554602706686318\nLearning rate: 0.25600000000000006\nTime step 120: Utility 0.07883403361344536\nLearning rate: 0.20480000000000007\nTime step 121: Utility 0.07198482428115015\nLearning rate: 0.20480000000000007\nTime step 122: Utility 0.06673048989832298\nLearning rate: 0.20480000000000007\nTime step 123: Utility 0.07350927997893904\nLearning rate: 0.20480000000000007\nTime step 124: Utility 0.0707703336809176\nLearning rate: 0.20480000000000007\nTime step 125: Utility 0.06923128253036163\nLearning rate: 0.20480000000000007\nTime step 126: Utility 0.07903606557377048\nLearning rate: 0.20480000000000007\nTime step 127: Utility 0.0791077044025157\nLearning rate: 0.20480000000000007\nTime step 128: Utility 0.07124452118475227\nLearning rate: 0.20480000000000007\nTime step 129: Utility 0.06487537913754449\nLearning rate: 0.20480000000000007\nTime step 130: Utility 0.07584284576586141\nLearning rate: 0.20480000000000007\nTime step 131: Utility 0.07343443587397043\nLearning rate: 0.20480000000000007\nTime step 132: Utility 0.06836571732765502\nLearning rate: 0.20480000000000007\nTime step 133: Utility 0.07725949450840278\nLearning rate: 0.20480000000000007\nTime step 134: Utility 0.07277665995975853\nLearning rate: 0.20480000000000007\nTime step 135: Utility 0.07505899318300994\nLearning rate: 0.20480000000000007\nTime step 136: Utility 0.07300052273915315\nLearning rate: 0.20480000000000007\nTime step 137: Utility 0.06811288408281681\nLearning rate: 0.20480000000000007\nTime step 138: Utility 0.08276021690252611\nLearning rate: 0.20480000000000007\nTime step 139: Utility 0.06940169806314671\nLearning rate: 0.20480000000000007\nTime step 140: Utility 0.0761345327604726\nLearning rate: 0.20480000000000007\nTime step 141: Utility 0.07757420541108483\nLearning rate: 0.20480000000000007\nTime step 142: Utility 0.0768758344459279\nLearning rate: 0.20480000000000007\nTime step 143: Utility 0.07284860557768923\nLearning rate: 0.20480000000000007\nTime step 144: Utility 0.07747699693292437\nLearning rate: 0.20480000000000007\nTime step 145: Utility 0.0634784896724115\nLearning rate: 0.20480000000000007\nTime step 146: Utility 0.07398724930269622\nLearning rate: 0.20480000000000007\nTime step 147: Utility 0.0668206062182906\nLearning rate: 0.20480000000000007\nTime step 148: Utility 0.07636783124588001\nLearning rate: 0.20480000000000007\nTime step 149: Utility 0.07486425638988212\nLearning rate: 0.20480000000000007\nTime step 150: Utility 0.06857199367088605\nLearning rate: 0.16384000000000007\nTime step 151: Utility 0.07302553247784097\nLearning rate: 0.16384000000000007\nTime step 152: Utility 0.07026191102921821\nLearning rate: 0.16384000000000007\nTime step 153: Utility 0.07376897689768977\nLearning rate: 0.16384000000000007\nTime step 154: Utility 0.07240390879478825\nLearning rate: 0.16384000000000007\nTime step 155: Utility 0.07382515095825674\nLearning rate: 0.16384000000000007\nTime step 156: Utility 0.07720329818869963\nLearning rate: 0.16384000000000007\nTime step 157: Utility 0.07297923005688581\nLearning rate: 0.16384000000000007\nTime step 158: Utility 0.08058910359634995\nLearning rate: 0.16384000000000007\nTime step 159: Utility 0.07070173120126866\nLearning rate: 0.16384000000000007\nTime step 160: Utility 0.06468758273762243\nLearning rate: 0.16384000000000007\nTime step 161: Utility 0.07199653702717101\nLearning rate: 0.16384000000000007\nTime step 162: Utility 0.07065969954278248\nLearning rate: 0.16384000000000007\nTime step 163: Utility 0.07394618239660657\nLearning rate: 0.16384000000000007\nTime step 164: Utility 0.08103699774565705\nLearning rate: 0.16384000000000007\nTime step 165: Utility 0.07204803202134753\nLearning rate: 0.16384000000000007\nTime step 166: Utility 0.081446377564269\nLearning rate: 0.16384000000000007\nTime step 167: Utility 0.07814136125654449\nLearning rate: 0.16384000000000007\nTime step 168: Utility 0.07688926790868537\nLearning rate: 0.16384000000000007\nTime step 169: Utility 0.06821705426356588\nLearning rate: 0.16384000000000007\nTime step 170: Utility 0.07650642299033238\nLearning rate: 0.16384000000000007\nTime step 171: Utility 0.06699144173798549\nLearning rate: 0.16384000000000007\nTime step 172: Utility 0.08384190451137707\nLearning rate: 0.16384000000000007\nTime step 173: Utility 0.06961016725931778\nLearning rate: 0.16384000000000007\nTime step 174: Utility 0.08325593737561361\nLearning rate: 0.16384000000000007\nTime step 175: Utility 0.07785084834933577\nLearning rate: 0.16384000000000007\nTime step 176: Utility 0.08480463096960925\nLearning rate: 0.16384000000000007\nTime step 177: Utility 0.08788632792037715\nLearning rate: 0.16384000000000007\nTime step 178: Utility 0.07691294613459766\nLearning rate: 0.16384000000000007\nTime step 179: Utility 0.08191115164147991\nLearning rate: 0.16384000000000007\nTime step 180: Utility 0.07479257210588698\nLearning rate: 0.13107200000000005\nTime step 181: Utility 0.0757033416985867\nLearning rate: 0.13107200000000005\nTime step 182: Utility 0.07396496815286623\nLearning rate: 0.13107200000000005\nTime step 183: Utility 0.07590170432025363\nLearning rate: 0.13107200000000005\nTime step 184: Utility 0.07457169214549288\nLearning rate: 0.13107200000000005\nTime step 185: Utility 0.0807882414151925\nLearning rate: 0.13107200000000005\nTime step 186: Utility 0.07579182546983833\nLearning rate: 0.13107200000000005\nTime step 187: Utility 0.06856223175965663\nLearning rate: 0.13107200000000005\nTime step 188: Utility 0.07546918674297882\nLearning rate: 0.13107200000000005\nTime step 189: Utility 0.08664529914529914\nLearning rate: 0.13107200000000005\nTime step 190: Utility 0.08154613466334162\nLearning rate: 0.13107200000000005\nTime step 191: Utility 0.06994326428288691\nLearning rate: 0.13107200000000005\nTime step 192: Utility 0.077902646628541\nLearning rate: 0.13107200000000005\nTime step 193: Utility 0.07467669569807335\nLearning rate: 0.13107200000000005\nTime step 194: Utility 0.07339185117254027\nLearning rate: 0.13107200000000005\nTime step 195: Utility 0.07569710585436763\nLearning rate: 0.13107200000000005\nTime step 196: Utility 0.07756666666666663\nLearning rate: 0.13107200000000005\nTime step 197: Utility 0.07864768683274019\nLearning rate: 0.13107200000000005\nTime step 198: Utility 0.08228042116486738\nLearning rate: 0.13107200000000005\nTime step 199: Utility 0.0779838389190621\nLearning rate: 0.13107200000000005\nTime step 200: Utility 0.0774709302325581\nLearning rate: 0.13107200000000005\n"
    }
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}