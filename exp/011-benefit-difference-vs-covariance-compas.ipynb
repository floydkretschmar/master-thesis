{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a comparison of COMPAS data across all fairness functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "from src.util import mean_difference\n",
    "from src.feature_map import IdentityFeatureMap\n",
    "from src.functions import cost_utility\n",
    "from src.plotting import plot_mean, plot_median\n",
    "from src.training import train\n",
    "from src.distribution import COMPASDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benefit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def calc_benefit(decisions, y, ips_weights):\n",
    "    if ips_weights is not None:\n",
    "        decisions *= ips_weights\n",
    "\n",
    "    return decisions\n",
    "\n",
    "def fairness_gradient_function(**fairness_kwargs):\n",
    "    policy = fairness_kwargs[\"policy\"]\n",
    "    x = fairness_kwargs[\"x\"]\n",
    "    s = fairness_kwargs[\"s\"]\n",
    "    y = fairness_kwargs[\"y\"]\n",
    "    decisions = fairness_kwargs[\"decisions\"]\n",
    "    ips_weights = fairness_kwargs[\"ips_weights\"]\n",
    "    benefit = calc_benefit(decisions, y, ips_weights)\n",
    "\n",
    "    log_gradient = policy.log_policy_gradient(x, s)\n",
    "    benefit_grad = log_gradient * benefit\n",
    "        \n",
    "    return mean_difference(benefit_grad, s)\n",
    "\n",
    "def fairness_function(**fairness_kwargs):\n",
    "    s = fairness_kwargs[\"s\"]\n",
    "    y = fairness_kwargs[\"y\"]\n",
    "    decisions = fairness_kwargs[\"decisions\"]\n",
    "    ips_weights = fairness_kwargs[\"ips_weights\"]\n",
    "    benefit = calc_benefit(decisions, y, ips_weights)\n",
    "        \n",
    "    return mean_difference(benefit, s)\n",
    "\n",
    "bias = True\n",
    "# distribution = ResamplingDistribution(bias=bias, dataset=load_dataset(\"../dat/compas/compas.npz\"), test_percentage=0.2)\n",
    "distribution = COMPASDistribution(bias=bias, test_percentage=0.2)\n",
    "dim_theta = distribution.feature_dim\n",
    "\n",
    "print(dim_theta)\n",
    "\n",
    "def util_func(**util_params):\n",
    "    util = cost_utility(cost_factor=0.5, **util_params)\n",
    "    return util\n",
    "\n",
    "training_parameters = {    \n",
    "    'model':{\n",
    "        'utility_function': util_func,\n",
    "        'fairness_function': fairness_function,\n",
    "        'fairness_gradient_function': fairness_gradient_function,\n",
    "        'feature_map': IdentityFeatureMap(dim_theta),\n",
    "        'learn_on_entire_history': False,\n",
    "        'use_sensitve_attributes': False,\n",
    "        'bias': bias,\n",
    "        'initial_theta': np.zeros((dim_theta))\n",
    "    },\n",
    "    'distribution': distribution,\n",
    "    'parameter_optimization': {\n",
    "        'time_steps':200,\n",
    "        'epochs': 1,\n",
    "        'batch_size':512,\n",
    "        'learning_rate': 0.1,\n",
    "        'decay_rate': 1,\n",
    "        'decay_step': 10000,\n",
    "        'num_batches': 256,\n",
    "        'fix_seeds': True\n",
    "    },\n",
    "    'test': {\n",
    "        'num_samples': 8192\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## STARTED Single training run // LR = 0.01 // TS = 200 // E = 1 // BS = 512 // NB = 256 ##\n",
      "## ENDED Single training run // LR = 0.01 // TS = 200 // E = 1 // BS = 512 // NB = 256 ##\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.training_evaluation import Statistics\n",
    "\n",
    "training_parameters[\"save_path\"] = \"./exp-011-compas-no-fairness\"\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = 0.0\n",
    "\n",
    "statistics, _, run_path = train(training_parameters, iterations=30, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_time.png\".format(run_path))\n",
    "plot_median(statistics, \"{}/results_median_time.png\".format(run_path))\n",
    "\n",
    "plt.plot(statistics.results[Statistics.X_VALUES], statistics.performance(Statistics.ACCURACY, Statistics.MEDIAN))\n",
    "plt.savefig(\"{}/acc.png\".format(run_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training with fixed lambdas ----------\n"
     ]
    }
   ],
   "source": [
    "training_parameters[\"save_path\"] = \"./exp-011-compas-benefit-lambda-sweep\"\n",
    "lambdas = np.logspace(-4, 0, base=10, endpoint=True, num=19)\n",
    "lambdas = np.insert(arr=lambdas, obj=0, values=[0.0])\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = lambdas\n",
    "\n",
    "statistics, model_parameters, run_path = train(training_parameters, iterations=5, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_lambdas.png\".format(run_path))\n",
    "plot_median(statistics, \"{}/results_median_lambdas.png\".format(run_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "training_parameters[\"save_path\"] = \"./exp-011-compas-benefit-dual-gradient\"\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = 0.0\n",
    "training_parameters[\"lagrangian_optimization\"] = {\n",
    "    'iterations': 300,\n",
    "    'epochs': 1,\n",
    "    'batch_size':256,\n",
    "    'learning_rate': 0.01,\n",
    "    'decay_rate': 1,\n",
    "    'decay_step': 1000000,\n",
    "    'num_decisions': 128 * 512\n",
    "}\n",
    "\n",
    "statistics, model_parameters, run_path = train(training_parameters, iterations=30, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_lambdas.png\".format(run_path), model_parameters=model_parameters)\n",
    "plot_median(statistics, \"{}/results_median_lambdas.png\".format(run_path), model_parameters=model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Covariance of decision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def calc_covariance(x, s, policy, ips_weights, decisions):    \n",
    "    new_s = 1 - (2*s)\n",
    "    \n",
    "    if ips_weights is not None:\n",
    "        mu_s = np.mean(new_s * ips_weights, axis=0)\n",
    "        decisions *= ips_weights\n",
    "    else:\n",
    "        mu_s = np.mean(new_s, axis=0)\n",
    "\n",
    "    covariance = (new_s - mu_s) * decisions\n",
    "    return covariance\n",
    "\n",
    "def fairness_gradient_function(**fairness_kwargs):\n",
    "    policy = fairness_kwargs[\"policy\"]\n",
    "    x = fairness_kwargs[\"x\"]\n",
    "    s = fairness_kwargs[\"s\"]\n",
    "    ips_weights = fairness_kwargs[\"ips_weights\"]\n",
    "    decisions = fairness_kwargs[\"decisions\"]\n",
    "    \n",
    "    covariance = calc_covariance(x, s, policy, ips_weights,decisions)\n",
    "\n",
    "    log_policy_gradient = policy.log_policy_gradient(x, s)\n",
    "    covariance_grad = log_policy_gradient * covariance\n",
    "\n",
    "    return np.mean(covariance_grad, axis=0)\n",
    "\n",
    "def fairness_function(**fairness_kwargs):\n",
    "    policy = fairness_kwargs[\"policy\"]\n",
    "    x = fairness_kwargs[\"x\"]\n",
    "    s = fairness_kwargs[\"s\"]\n",
    "    ips_weights = fairness_kwargs[\"ips_weights\"]\n",
    "    decisions = fairness_kwargs[\"decisions\"]\n",
    "    \n",
    "    covariance = calc_covariance(x, s, policy, ips_weights,decisions)\n",
    "    return np.mean(covariance, axis=0)\n",
    "\n",
    "bias = True\n",
    "distribution = COMPASDistribution(bias=bias, test_percentage=0.2)\n",
    "dim_theta = distribution.feature_dim\n",
    "\n",
    "def util_func(**util_params):\n",
    "    util = cost_utility(cost_factor=0.5, **util_params)\n",
    "    return util\n",
    "\n",
    "training_parameters = {    \n",
    "    'model':{\n",
    "        'utility_function': util_func,\n",
    "        'fairness_function': fairness_function,\n",
    "        'fairness_gradient_function': fairness_gradient_function,\n",
    "        'feature_map': IdentityFeatureMap(dim_theta),\n",
    "        'learn_on_entire_history': False,\n",
    "        'use_sensitve_attributes': False,\n",
    "        'bias': bias,\n",
    "        'initial_theta': np.zeros((dim_theta))\n",
    "    },\n",
    "    'distribution': distribution,\n",
    "    'parameter_optimization': {\n",
    "        'time_steps':200,\n",
    "        'epochs': 1,\n",
    "        'batch_size':512,\n",
    "        'learning_rate': 0.01,\n",
    "        'decay_rate': 1,\n",
    "        'decay_step': 10000,\n",
    "        'num_batches': 256,\n",
    "        'fix_seeds': True\n",
    "    },\n",
    "    'test': {\n",
    "        'num_samples': 8192\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training with fixed lambdas ----------\n"
     ]
    }
   ],
   "source": [
    "training_parameters[\"save_path\"] = \"./exp-011-compas-covdecision-lambda-sweep\"\n",
    "lambdas = np.logspace(-6, 0, base=10, endpoint=True, num=10)\n",
    "#lambdas = np.insert(arr=lambdas, obj=0, values=[0.0])\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = lambdas\n",
    "\n",
    "statistics, _, run_path = train(training_parameters, iterations=5, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_lambdas.png\".format(run_path))\n",
    "plot_median(statistics, \"{}/results_median_lambdas.png\".format(run_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_parameters[\"experiment_name\"] = \"exp-011-compas-covdecision-dual-gradient\"\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = 0.0\n",
    "training_parameters[\"lagrangian_optimization\"] = {\n",
    "    'iterations': 20,\n",
    "    'epochs': 10,\n",
    "    'batch_size':256,\n",
    "    'learning_rate': 0.01,\n",
    "    'decay_rate': 1,\n",
    "    'decay_step': 10000,\n",
    "    'num_decisions': 128 * 256\n",
    "}\n",
    "\n",
    "statistics, model_parameters, run_path = train(training_parameters, iterations=30, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_lambdas.png\".format(run_path), model_parameters=model_parameters)\n",
    "plot_median(statistics, \"{}/results_median_lambdas.png\".format(run_path), model_parameters=model_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benefit difference (equality of opportunity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_benefit(decisions, y, ips_weights):\n",
    "    if ips_weights is not None:\n",
    "        decisions *= ips_weights\n",
    "\n",
    "    return decisions\n",
    "\n",
    "def fairness_gradient_function(**fairness_kwargs):\n",
    "    policy = fairness_kwargs[\"policy\"]\n",
    "    x = fairness_kwargs[\"x\"]\n",
    "    s = fairness_kwargs[\"s\"]\n",
    "    y = fairness_kwargs[\"y\"]\n",
    "    decisions = fairness_kwargs[\"decisions\"]\n",
    "    ips_weights = fairness_kwargs[\"ips_weights\"]\n",
    "    benefit = calc_benefit(decisions, y, ips_weights)\n",
    "    \n",
    "    y1_indices = np.where(y == 1)\n",
    "\n",
    "    log_gradient = policy.log_policy_gradient(x, s)\n",
    "    benefit_grad = log_gradient * benefit\n",
    "        \n",
    "    return mean_difference(benefit_grad[y1_indices], s[y1_indices])\n",
    "\n",
    "def fairness_function(**fairness_kwargs):\n",
    "    s = fairness_kwargs[\"s\"]\n",
    "    y = fairness_kwargs[\"y\"]\n",
    "    decisions = fairness_kwargs[\"decisions\"]\n",
    "    ips_weights = fairness_kwargs[\"ips_weights\"]\n",
    "    benefit = calc_benefit(decisions, y, ips_weights)\n",
    "    \n",
    "    y1_indices = np.where(y == 1)\n",
    "        \n",
    "    return mean_difference(benefit[y1_indices], s[y1_indices])\n",
    "\n",
    "bias = True\n",
    "distribution = COMPASDistribution(bias=bias, test_percentage=0.2)\n",
    "dim_theta = distribution.feature_dim\n",
    "\n",
    "def util_func(**util_params):\n",
    "    util = cost_utility(cost_factor=0.6, **util_params)\n",
    "    return util\n",
    "\n",
    "training_parameters = {    \n",
    "    'model':{\n",
    "        'utility_function': util_func,\n",
    "        'fairness_function': fairness_function,\n",
    "        'fairness_gradient_function': fairness_gradient_function,\n",
    "        'feature_map': IdentityFeatureMap(dim_theta),\n",
    "        'learn_on_entire_history': False,\n",
    "        'use_sensitve_attributes': False,\n",
    "        'bias': bias,\n",
    "        'initial_theta': np.zeros((dim_theta))\n",
    "    },\n",
    "    'distribution': distribution,\n",
    "    'parameter_optimization': {\n",
    "        'time_steps':200,\n",
    "        'epochs': 1,\n",
    "        'batch_size':512,\n",
    "        'learning_rate': 0.1,\n",
    "        'decay_rate': 1,\n",
    "        'decay_step': 10000,\n",
    "        'num_batches': 128,\n",
    "        'fix_seeds': True\n",
    "    },\n",
    "    'test': {\n",
    "        'num_samples': 8192\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_parameters[\"save_path\"] = \"./exp-011-compas-benefit-lambda-sweep-eop\"\n",
    "lambdas = np.logspace(-2, 2, base=10, endpoint=True, num=19)\n",
    "lambdas = np.insert(arr=lambdas, obj=0, values=[0.0])\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = lambdas\n",
    "\n",
    "statistics, _, run_path = train(training_parameters, iterations=10, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_lambdas.png\".format(run_path))\n",
    "plot_median(statistics, \"{}/results_median_lambdas.png\".format(run_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_parameters[\"save_path\"] = \"./exp-011-compas-benefit-dual-gradient-eop\"\n",
    "training_parameters[\"model\"][\"initial_lambda\"] = 0.0\n",
    "training_parameters[\"lagrangian_optimization\"] = {\n",
    "    'iterations': 20,\n",
    "    'epochs': 1,\n",
    "    'batch_size':256,\n",
    "    'learning_rate': 1,\n",
    "    'decay_rate': 1,\n",
    "    'decay_step': 10000,\n",
    "    'num_decisions': 128 * 256\n",
    "}\n",
    "\n",
    "statistics, model_parameters, run_path = train(training_parameters, iterations=5, asynchronous=True)\n",
    "\n",
    "plot_mean(statistics, \"{}/results_mean_lambdas.png\".format(run_path), model_parameters=model_parameters)\n",
    "plot_median(statistics, \"{}/results_median_lambdas.png\".format(run_path), model_parameters=model_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}